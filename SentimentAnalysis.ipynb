{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "784ab1bc",
   "metadata": {},
   "source": [
    "# Data preparation for sentiment analysis model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aee0d5d",
   "metadata": {},
   "source": [
    "First we import libraries to manage data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35be4bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc80a8bb",
   "metadata": {},
   "source": [
    "Counter function from collections library will be used to count the words in the different review and then this object will be used to define an encoder to converte string to number array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a728e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d089e4aa",
   "metadata": {},
   "source": [
    "The used dataset contains film's reviews from IMDB. Each review is labelled as \"positive\" or \"negative\" and we are going to use this data to build a supervised Recurrent neural network (RNN) to accomplish a sentiment analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b471291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = pd.read_csv(\"IMDB.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5a18ba",
   "metadata": {},
   "source": [
    "We convert from string type to numeric the sentiment in the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1c7346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb.loc[imdb.sentiment == \"negative\", \"score\"] = 0\n",
    "imdb.loc[imdb.sentiment == \"positive\", \"score\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f14d048d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  score\n",
       "0  One of the other reviewers has mentioned that ...  positive    1.0\n",
       "1  A wonderful little production. <br /><br />The...  positive    1.0\n",
       "2  I thought this was a wonderful way to spend ti...  positive    1.0\n",
       "3  Basically there's a family where a little boy ...  negative    0.0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive    1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5d2ce6",
   "metadata": {},
   "source": [
    "The first thing to do is create a tensorflow dataset object and defined the train/test/valid set to validate the model through a holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "764ec12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_raw = tf.data.Dataset.from_tensor_slices((imdb.review, imdb.score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dc76d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_raw = ds_raw.shuffle(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dd63126",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_raw_test = ds_raw.take(25000)\n",
    "ds_raw_train_valid = ds_raw.skip(25000)\n",
    "ds_raw_train = ds_raw_train_valid.take(20000)\n",
    "ds_raw_valid = ds_raw_train_valid.skip(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbd5c28",
   "metadata": {},
   "source": [
    "To encode the string reviews we use the previously imported Counter to store every word used in the reviews. \n",
    "the reviews are made readable for the counter using the Tokenizer function provided in the Tensorflow library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "262b37d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter()\n",
    "tokenizer = tfds.deprecated.text.Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "391da981",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rew in ds_raw_train:\n",
    "    token = tokenizer.tokenize(rew[0].numpy())\n",
    "    count.update(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13dd583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tfds.deprecated.text.TokenTextEncoder(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c55423",
   "metadata": {},
   "source": [
    "Once the reading of the reviews has been completed it is possible to decode them into numerical arrays as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce2cb178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[361, 61, 75, 431]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.encode(\"This is an example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f32a52f",
   "metadata": {},
   "source": [
    "At this point we could define a function to encode the sets previously built. It's essential to convert it to a tensorflow function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64294754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tensor, label):\n",
    "    text = tensor.numpy()\n",
    "    encoded_text = encoder.encode(text)\n",
    "    return encoded_text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd25b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_encode(tensor, label):\n",
    "    return tf.py_function(encode, inp = [tensor, label], Tout = (tf.int64, tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d94f8b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_raw_train.map(tf_encode)\n",
    "ds_test = ds_raw_test.map(tf_encode)\n",
    "ds_valid = ds_raw_valid.map(tf_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a12286",
   "metadata": {},
   "source": [
    "As can be expected the vectors of the regressors are of different sizes. In general RNN can also handle several dimensions but you can simplify this by collecting reviews in batches and using padding to have the same size in each collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d0f3e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ds_train.padded_batch(32, padded_shapes = ([-1], []) )\n",
    "test = ds_test.padded_batch(32, padded_shapes = ([-1], []) )\n",
    "valid = ds_valid.padded_batch(32, padded_shapes = ([-1], []) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab22a9f5",
   "metadata": {},
   "source": [
    "Finally we have the dataset processed to be used efficently in a RNN model. \n",
    "The model will be buld in the follow fashion:  \n",
    "$\\bullet$ embedding layer: maps discrete numerical array to continous array and normalized  \n",
    "$\\bullet$ bidirectional LSTM layer: LSTM neural network in which the recursion is forward and backward. The chosen activation function is the hyperbolic tangent. Finally a dropout with 0.2 rate is set in this layer.  \n",
    "$\\bullet$ dense hidden layer with 64 neurons  \n",
    "$\\bullet$ Output dense layer with sigmoid activation to classify review in feed-forward  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b25be37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Embedding(input_dim = len(count)+2, \n",
    "                                    output_dim = 20))\n",
    "\n",
    "model.add(tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(64, activation = \"tanh\", dropout = 0.2))\n",
    "         )\n",
    "\n",
    "model.add(tf.keras.layers.Dense(64, activation = \"tanh\"))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bc3625e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 20)          1745120   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               43520     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 1,796,961\n",
      "Trainable params: 1,796,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0357eb80",
   "metadata": {},
   "source": [
    "The optimization method chosen to minime the binary crossentropy loss function is the \"adam\" stochastic gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13a04cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a3f06a",
   "metadata": {},
   "source": [
    "To simplify the times of execution on a local machine only ten epochs of learning are carried out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5697f750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 260s 410ms/step - loss: 0.4802 - accuracy: 0.7613 - val_loss: 0.3013 - val_accuracy: 0.8804\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 249s 397ms/step - loss: 0.3192 - accuracy: 0.8709 - val_loss: 0.2153 - val_accuracy: 0.9250\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 253s 405ms/step - loss: 0.2673 - accuracy: 0.8989 - val_loss: 0.2037 - val_accuracy: 0.9306\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 267s 428ms/step - loss: 0.2249 - accuracy: 0.9202 - val_loss: 0.1801 - val_accuracy: 0.9356\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 277s 443ms/step - loss: 0.1914 - accuracy: 0.9321 - val_loss: 0.1559 - val_accuracy: 0.9524\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 290s 464ms/step - loss: 0.1520 - accuracy: 0.9471 - val_loss: 0.1110 - val_accuracy: 0.9668\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 275s 440ms/step - loss: 0.1497 - accuracy: 0.9490 - val_loss: 0.0946 - val_accuracy: 0.9690\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 372s 595ms/step - loss: 0.3090 - accuracy: 0.8616 - val_loss: 0.5396 - val_accuracy: 0.7476\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 272s 434ms/step - loss: 0.4036 - accuracy: 0.8217 - val_loss: 0.3783 - val_accuracy: 0.8514\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 351s 562ms/step - loss: 0.2651 - accuracy: 0.9007 - val_loss: 0.1646 - val_accuracy: 0.9440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20c65469430>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train, validation_data = valid, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaf3615",
   "metadata": {},
   "source": [
    "In the end it's possible to make predictions on records not yet seen by the model and evaluate the generalized error with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b05f9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 120s 153ms/step - loss: 0.1577 - accuracy: 0.9501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1576797217130661, 0.9500799775123596]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61c920d",
   "metadata": {},
   "source": [
    "The values of generalized error, of training and on the test of validation are in all the cases very low and therefore the risk of overfitting can be considered averted and the model can be considered satisfactory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
